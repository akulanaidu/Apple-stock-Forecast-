# -*- coding: utf-8 -*-
"""AAPL_model_deploy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mD2fTrUQ4HOq1snzybyLA_v06RxMOoFg

# stock market analysis

### Import necessary libraries
"""
import streamlit as st
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error,r2_score
import math
from keras.models import Sequential
from keras.layers import LSTM, Dense
import matplotlib.pyplot as plt

"""### loading dataset"""

# Load the CSV file
df = pd.read_csv("AAPL.csv")
df

# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')

# Sort the data by date
df = df.sort_values('Date')

# Set the 'Date' column as the index
df.set_index('Date', inplace=True)

# Extract the year from the 'Date' index
df['Year'] = df.index.year

"""# outliers handiling"""

from sklearn.ensemble import IsolationForest

# Detecting outliers using Isolation Forest
iso_forest = IsolationForest(contamination=0.01)
df['Outliers'] = iso_forest.fit_predict(df[['Close']])

# Counting the number of outliers
outliers_count = (df['Outliers'] == -1).sum()

# Printing the number of outliers
print(f'Number of outliers: {outliers_count}')
st.write(f'Number of outliers: {outliers_count}')

# Removing outliers
df = df[df['Outliers'] == 1]
df.drop(columns=['Outliers'], inplace=True)

# Apply differencing to achieve stationarity
differenced_series = df['Close'].diff().dropna()

"""#  Data standrdization & Data partition"""

# Split the data into training and testing sets based on the year
train_data = df[df['Year'] < 2019]
test_data = df[df['Year'] == 2019]

# Use only the 'Close' prices for forecasting
train_close_prices = train_data['Close'].values.reshape(-1, 1)
test_close_prices = test_data['Close'].values.reshape(-1, 1)

# Standardize the data
scaler = StandardScaler()
train_scaled_close_prices = scaler.fit_transform(train_close_prices)
test_scaled_close_prices = scaler.transform(test_close_prices)

# Recheck stationarity with ADF and KPSS tests after transformation

# Apply differencing to achieve stationarity
differenced_series = df['Close'].diff().dropna()



"""Based on the results of the Augmented Dickey-Fuller (ADF) test and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, the data is stationary.

Summary of Results:
ADF Test:

ADF Statistic: -13.626
p-value: 1.7631e-25
Interpretation: The p-value is extremely small (far less than 0.05), which means we reject the null hypothesis of a unit root. This suggests that the time series is stationary according to the ADF test.
KPSS Test:

KPSS Statistic: 0.377
p-value: 0.087
Interpretation: The p-value is relatively high (greater than 0.05), which means we fail to reject the null hypothesis of stationarity around a deterministic trend. This further confirms that the time series is stationary according to the KPSS test.
Conclusion:
Yes, the data is stationary.

Plain Statement:
The data is stationary.

This conclusion is derived from the results of both the ADF and KPSS tests, which collectively confirm the stationarity of the time series.
"""

# Convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return np.array(dataX), np.array(dataY)

time_step = 100
X_train, Y_train = create_dataset(train_scaled_close_prices, time_step)
X_test, Y_test = create_dataset(test_scaled_close_prices, time_step)

# Reshape input to be [samples, time steps, features]
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

"""# LSTM model

"""

# Create the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, Y_train, batch_size=1, epochs=1)

# Make predictions
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Transform back to original form
train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)
Y_train = scaler.inverse_transform([Y_train])
Y_test = scaler.inverse_transform([Y_test])

# Calculate root mean squared error
train_score = math.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))
test_score = math.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))

# Calculate R-squared
train_r2 = r2_score(Y_train[0], train_predict[:,0])
test_r2 = r2_score(Y_test[0], test_predict[:,0])

print(f'Train Score: {train_score} RMSE')
print(f'Test Score: {test_score} RMSE')
print(f'Train R²: {train_r2}')
print(f'Test R²: {test_r2}')

# Streamlit app
st.title('LSTM Model Deployment')
st.write(f'Train Score: {train_score} RMSE')
st.write(f'Test Score: {test_score} RMSE')
st.write(f'Train R²: {train_r2}')
st.write(f'Test R²: {test_r2}')

#
# Predict the next 30 days
def predict_next_days(model, data, scaler, time_step=100, days_to_predict=30):
    # Prepare the data to start prediction
    last_data_points = data[-time_step:].reshape(1, time_step, 1)  # Reshape for model input
    predictions = []

    for _ in range(days_to_predict):
        next_pred = model.predict(last_data_points)
        predictions.append(next_pred[0, 0])

        # Update the last_data_points by removing the first element and adding the new prediction
        next_pred_reshaped = next_pred.reshape(1, 1, 1)  # Reshape next_pred to be [samples, time steps, features]
        last_data_points = np.append(last_data_points[:, 1:, :], next_pred_reshaped, axis=1)

    # Inverse transform the predictions to get actual values
    predictions = np.array(predictions).reshape(-1, 1)
    predictions = scaler.inverse_transform(predictions)

    return predictions

days_to_predict = 30
next_30_days_predictions = predict_next_days(model, test_scaled_close_prices, scaler, time_step, days_to_predict)

# Print the predicted values for the next 30 days
print("Predictions for the next 30 days:")
for i, pred in enumerate(next_30_days_predictions, 1):
    print(f"Day {i}: {pred[0]}")


# Plotting test data
fig, ax = plt.subplots()
ax.plot(Y_test[0], label='Actual Test Data')
ax.plot(test_predict[:,0], label='Predicted Test Data')
ax.legend()
st.pyplot(fig)

# Display the next 30 days predictions
st.subheader('Predictions for the Next 30 Days')
predictions_df = pd.DataFrame(next_30_days_predictions, columns=['Predicted Values'])
st.write(predictions_df)

# Plotting the next 30 days predictions
fig, ax = plt.subplots()
ax.plot(predictions_df, label='Next 30 Days Predictions')
ax.legend()
st.pyplot(fig)

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt


# Plot the data
def plot_predictions(actual, train_predict, test_predict, future_predictions, time_step, days_to_predict):
    plt.figure(figsize=(14, 8))

    # Plot actual values
    plt.plot(actual, label='Actual Data', color='blue')

    # Plot training predictions
    train_size = len(train_predict)
    train_plot = np.empty_like(actual)
    train_plot[:, :] = np.nan
    train_plot[time_step:train_size+time_step] = train_predict
    plt.plot(train_plot, label='Training Predictions', color='green')

    # Plot testing predictions
    test_size = len(test_predict)
    test_start_idx = train_size + (time_step * 2) + 1
    test_end_idx = test_start_idx + test_size
    
    # Ensure indices do not go out of bounds
    if test_end_idx > len(actual):
        test_end_idx = len(actual)
        test_predict = test_predict[:(test_end_idx - test_start_idx)]

    test_plot = np.empty_like(actual)
    test_plot[:, :] = np.nan
    test_plot[test_start_idx:test_end_idx] = test_predict
    
    plt.plot(test_plot, label='Testing Predictions', color='red')

    # Plot future predictions
    future_plot = np.empty_like(actual)
    future_plot[:, :] = np.nan
    future_plot[-days_to_predict:] = future_predictions
    plt.plot(range(len(actual), len(actual) + days_to_predict), future_predictions, label='30 Days Predictions', color='purple')

    plt.legend()
    plt.xlabel('Time')
    plt.ylabel('Price')
    plt.title('Stock Price Predictions')
    st.pyplot(plt)

# Combine the training and test sets for plotting actual values
all_data = np.concatenate((train_scaled_close_prices, test_scaled_close_prices), axis=0)
actual = scaler.inverse_transform(all_data)

# Call the function to plot the predictions
plot_predictions(actual, train_predict, test_predict, next_30_days_predictions, time_step, days_to_predict)
